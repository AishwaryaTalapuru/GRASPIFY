# GRASPIFY
Adaptive Context Aware Answering Engine for Multimodal data

1. Create virtual envirment 

source venv/bin/activate

2. run "python main.py"

(This will take data from all the three modalities and store the data with embeddings in "transcription/data.json" file)
(You need not run this if you are using the same input files)

3. run "python analyse_similarity.py"
(This will provide unlimited querying scopes till you choose to quit)
